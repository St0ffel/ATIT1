{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common / KNN imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.spatial.distance\n",
    "from collections import Counter\n",
    "plt.set_cmap('gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgaben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many distances you need to calculate if you have 60,000 samples in the trainingset for 50 samples?\n",
    "Man muss 3.000.000 Abstände berechnen.\n",
    "#### How many distances do you need to calculate if you have n samples im the trainingset?\n",
    "Man muss immer n * 50 Abstände berechnen, d.h. bei 3 Samples müsste man 150 Abstände berechnen, bei 90.000 dann 4.500.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(k, x_train, y_train, x_test):\n",
    "    dists = scipy.spatial.distance.cdist(x_train, x_test, metric='euclid')\n",
    "    idx_nearest = np.argpartition(dists, k, axis=0)[:k]\n",
    "    idx_nearest = np.swapaxes(idx_nearest, 0, 1)\n",
    "    #print(idx_nearest)\n",
    "\n",
    "    closest_points_labels = y_train[idx_nearest]\n",
    "    y_test_predictions = []\n",
    "    \n",
    "    for index, predicted_labels in enumerate(closest_points_labels):\n",
    "        y_test_predictions.append(np.bincount(predicted_labels).argmax())\n",
    "    return y_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_print_error_rate(y_test,  y_test_predictions):\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    correct_labels = (y_test)\n",
    "    \n",
    "    numbers_count = Counter(correct_labels)\n",
    "    \n",
    "    numbers_correct = dict.fromkeys(numbers_count.keys(), 0)\n",
    "    \n",
    "    #numbers_error_rate = dict.fromkeys(numbers_count.keys(), None)\n",
    "    \n",
    "    \n",
    "    n = correct_labels.shape[0]\n",
    "    \n",
    "    for index, prediction in enumerate( y_test_predictions):\n",
    "        if prediction == correct_labels[index]:\n",
    "            correct += 1\n",
    "            numbers_correct[prediction] += 1\n",
    "        \n",
    "    error_rate = 1- (correct / n)\n",
    "    \n",
    "    \n",
    "    for key in sorted(numbers_count):\n",
    "        count = numbers_count[key]\n",
    "        correct = numbers_correct[key]\n",
    "        error = 1 - numbers_correct[key] / numbers_count[key]\n",
    "        print(\"Label:\", key)\n",
    "        print(\"  \", correct, \"out of\", count, \"recognized correctly | Errorrate:\", error*100, \"%\")\n",
    "        \n",
    "    print(\"\\n\"+\"Resulting errorrate:\", error_rate*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "mnist_data = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Get Training set\n",
    "train = mnist_data[0]\n",
    "X_train, Y_train = train[0], train[1]\n",
    "\n",
    "# Get Test set\n",
    "test = mnist_data[1]\n",
    "X_test, Y_test = test[0], test[1]\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "m = X_train.shape[1]**2\n",
    "\n",
    "X_train = X_train.reshape([n_train, m])\n",
    "X_test = X_test.reshape([n_test, m])\n",
    "\n",
    "# Create train subset\n",
    "idx = np.random.randint(n_train, size=10000)\n",
    "x_train_sample = X_train[idx]\n",
    "y_train_sample = Y_train[idx]\n",
    "\n",
    "# Create test subset\n",
    "idx = np.random.randint(n_test, size=30)\n",
    "x_test_sample = X_test[idx]\n",
    "y_test_sample = Y_test[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 8, 1, 1, 0, 3, 8, 4, 4, 5, 7, 1, 6, 0, 8, 3, 9, 9, 1, 2, 4, 1, 0, 9, 8, 6, 5, 9, 3] \n",
      "\n",
      "-------------------------------------\n",
      "  2NN  \n",
      "-------------------------------------\n",
      "Predictions:\n",
      "[3, 6, 8, 1, 1, 0, 3, 8, 4, 4, 9, 7, 1, 6, 0, 8, 3, 9, 9, 1, 2, 4, 1, 0, 9, 8, 6, 5, 4, 3] \n",
      "\n",
      "Label: 0\n",
      "   3 out of 3 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 1\n",
      "   5 out of 5 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 2\n",
      "   1 out of 1 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 3\n",
      "   4 out of 4 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 4\n",
      "   3 out of 3 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 5\n",
      "   1 out of 2 recognized correctly | Errorrate: 50.0 %\n",
      "Label: 6\n",
      "   3 out of 3 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 7\n",
      "   1 out of 1 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 8\n",
      "   4 out of 4 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 9\n",
      "   3 out of 4 recognized correctly | Errorrate: 25.0 %\n",
      "\n",
      "Resulting errorrate: 6.666666666666665 %\n",
      "-------------------------------------\n",
      "  4NN  \n",
      "-------------------------------------\n",
      "Predictions:\n",
      "[3, 6, 8, 1, 1, 0, 3, 8, 4, 4, 9, 7, 1, 6, 0, 8, 3, 9, 9, 1, 2, 4, 1, 0, 9, 8, 6, 5, 4, 3] \n",
      "\n",
      "Label: 0\n",
      "   3 out of 3 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 1\n",
      "   5 out of 5 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 2\n",
      "   1 out of 1 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 3\n",
      "   4 out of 4 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 4\n",
      "   3 out of 3 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 5\n",
      "   1 out of 2 recognized correctly | Errorrate: 50.0 %\n",
      "Label: 6\n",
      "   3 out of 3 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 7\n",
      "   1 out of 1 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 8\n",
      "   4 out of 4 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 9\n",
      "   3 out of 4 recognized correctly | Errorrate: 25.0 %\n",
      "\n",
      "Resulting errorrate: 6.666666666666665 %\n",
      "-------------------------------------\n",
      "  8NN  \n",
      "-------------------------------------\n",
      "Predictions:\n",
      "[3, 6, 8, 1, 1, 0, 3, 8, 4, 4, 9, 7, 1, 6, 0, 8, 3, 9, 9, 1, 2, 4, 1, 0, 9, 8, 6, 5, 4, 3] \n",
      "\n",
      "Label: 0\n",
      "   3 out of 3 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 1\n",
      "   5 out of 5 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 2\n",
      "   1 out of 1 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 3\n",
      "   4 out of 4 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 4\n",
      "   3 out of 3 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 5\n",
      "   1 out of 2 recognized correctly | Errorrate: 50.0 %\n",
      "Label: 6\n",
      "   3 out of 3 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 7\n",
      "   1 out of 1 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 8\n",
      "   4 out of 4 recognized correctly | Errorrate: 0.0 %\n",
      "Label: 9\n",
      "   3 out of 4 recognized correctly | Errorrate: 25.0 %\n",
      "\n",
      "Resulting errorrate: 6.666666666666665 %\n"
     ]
    }
   ],
   "source": [
    "predictions2 = knn(2, x_train_sample, y_train_sample, x_test_sample)\n",
    "predictions4 = knn(4, x_train_sample, y_train_sample, x_test_sample)\n",
    "predictions8 = knn(8, x_train_sample, y_train_sample, x_test_sample)\n",
    "\n",
    "print(list(y_test_sample), \"\\n\")\n",
    "\n",
    "print(\"-------------------------------------\")\n",
    "print(\"  2NN  \")\n",
    "print(\"-------------------------------------\")\n",
    "print(\"Predictions:\")\n",
    "print(predictions2, \"\\n\")\n",
    "knn_print_error_rate(y_test_sample, predictions2)\n",
    "\n",
    "print(\"-------------------------------------\")\n",
    "print(\"  4NN  \")\n",
    "print(\"-------------------------------------\")\n",
    "print(\"Predictions:\")\n",
    "print(predictions4, \"\\n\")\n",
    "knn_print_error_rate(y_test_sample,predictions4)\n",
    "\n",
    "print(\"-------------------------------------\")\n",
    "print(\"  8NN  \")\n",
    "print(\"-------------------------------------\")\n",
    "print(\"Predictions:\")\n",
    "print(predictions8, \"\\n\")\n",
    "knn_print_error_rate(y_test_sample,predictions8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the error rate of KNN on the test set?\n",
    "Die Errorrate beim ersten Testen (mit k=2, train = 10.000, test = 30) war 6%.  \n",
    "Beim testen mit neuen subsets gab es auch Errorraten zwischen 3 - 16%\n",
    "##### What is the error rate for each label (number)?\n",
    "Die Errorraten beim ersten Testen (mit k=2) waren:\n",
    "* 0 - 0.0%\n",
    "* 1 - 0.0%\n",
    "* 2 - 0.0%\n",
    "* 3 - 0.0%\n",
    "* 4 - 33.0% (2/3)\n",
    "* 5 - 20.0% (4/5)\n",
    "* 6 - 0.0%\n",
    "* 7 - 0.0%\n",
    "* 8 - 0.0%\n",
    "* 9 - 0.0%  \n",
    "  \n",
    "Beim testen mit anderen Subsets fällt auf, dass es bei den Zahlen 0-2 tendenziell niedrigere Fehlerquoten gibt als beim Rest.\n",
    "##### How does the choice of k influence the result?\n",
    "Beim ersten Testen hat sich die Errorrate mit zunehmendem k verbessert:\n",
    "\n",
    "* k = 2: 6%\n",
    "* k = 4: 3%\n",
    "* k = 8: 3%  \n",
    "  \n",
    "Beim Testen mit anderen Subsets ist aufgefallen, dass k sich nicht immer auf die Errorquote auswirkt, aber k=4 tendenziell niedrigere Errorraten für unsere Test- und Traingrößen hatte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C K-means MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Indexed 1000 samples in 0.050s...\n",
      "[t-SNE] Computed neighbors for 1000 samples in 3.293s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1000\n",
      "[t-SNE] Mean sigma: 480.240386\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 68.160133\n",
      "[t-SNE] KL divergence after 300 iterations: 1.372422\n"
     ]
    }
   ],
   "source": [
    "# load subset\n",
    "n_sample = 1000\n",
    "n = X_train.shape[0]\n",
    "\n",
    "idx = np.random.randint(n, size=n_sample)\n",
    "x_mnist = X_train[idx]\n",
    "y_mnist = Y_train[idx]\n",
    "\n",
    "k_mnist = 10\n",
    "it_mnist = 30\n",
    "_tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "embedding = _tsne.fit_transform(x_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_train(k, it, x_train, y_train):\n",
    "    \n",
    "    # init centroids\n",
    "    idx_centroid = np.random.randint(x_train.shape[0], size=k)\n",
    "    centroids = x_train[idx_centroid]\n",
    "    \n",
    "    # calc distances\n",
    "    dists = scipy.spatial.distance.cdist(x_train, centroids, metric='euclid')\n",
    "    \n",
    "    # find nearest centroids\n",
    "    labels = np.argmin(dists, axis=1)\n",
    "\n",
    "    # train model for q iterations\n",
    "    for j in range(it):\n",
    "        \n",
    "        # calc distances\n",
    "        dists = scipy.spatial.distance.cdist(x_train, centroids, metric='euclid')\n",
    "        \n",
    "        # label according to nearest centroids\n",
    "        labels = np.argmin(dists, axis=1)\n",
    "        \n",
    "        # get new centroids\n",
    "        centroid_list = [x_train[labels == i].mean(0) for i in range(k)]\n",
    "        centroids = np.stack(centroid_list)\n",
    "        \n",
    "        \n",
    "        # print results\n",
    "\n",
    "    for i in range(k):\n",
    "    \n",
    "        size = y_train[labels == i].shape[0]\n",
    "        bincount = np.bincount(y_train[labels == i])\n",
    "        mode = bincount.argmax()\n",
    "        correct = bincount[mode]\n",
    "        percentage = correct / size\n",
    "    \n",
    "        print('group', i)\n",
    "        print(\"-------------------------------------\")\n",
    "        print(\"label\", mode, \"(size:\", size, \")\")\n",
    "        print(\"correct\", correct)\n",
    "        print(\"size\", size)\n",
    "        print(\"Percentage of majority class:\", percentage*100, \"%\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group 0\n",
      "-------------------------------------\n",
      "label 7 (size: 155 )\n",
      "correct 56\n",
      "size 155\n",
      "Percentage of majority class: 36.12903225806451 % \n",
      "\n",
      "group 1\n",
      "-------------------------------------\n",
      "label 0 (size: 68 )\n",
      "correct 63\n",
      "size 68\n",
      "Percentage of majority class: 92.64705882352942 % \n",
      "\n",
      "group 2\n",
      "-------------------------------------\n",
      "label 6 (size: 75 )\n",
      "correct 49\n",
      "size 75\n",
      "Percentage of majority class: 65.33333333333333 % \n",
      "\n",
      "group 3\n",
      "-------------------------------------\n",
      "label 3 (size: 157 )\n",
      "correct 74\n",
      "size 157\n",
      "Percentage of majority class: 47.13375796178344 % \n",
      "\n",
      "group 4\n",
      "-------------------------------------\n",
      "label 2 (size: 74 )\n",
      "correct 73\n",
      "size 74\n",
      "Percentage of majority class: 98.64864864864865 % \n",
      "\n",
      "group 5\n",
      "-------------------------------------\n",
      "label 4 (size: 103 )\n",
      "correct 53\n",
      "size 103\n",
      "Percentage of majority class: 51.45631067961165 % \n",
      "\n",
      "group 6\n",
      "-------------------------------------\n",
      "label 5 (size: 82 )\n",
      "correct 29\n",
      "size 82\n",
      "Percentage of majority class: 35.36585365853659 % \n",
      "\n",
      "group 7\n",
      "-------------------------------------\n",
      "label 1 (size: 127 )\n",
      "correct 92\n",
      "size 127\n",
      "Percentage of majority class: 72.44094488188976 % \n",
      "\n",
      "group 8\n",
      "-------------------------------------\n",
      "label 8 (size: 122 )\n",
      "correct 47\n",
      "size 122\n",
      "Percentage of majority class: 38.52459016393443 % \n",
      "\n",
      "group 9\n",
      "-------------------------------------\n",
      "label 6 (size: 37 )\n",
      "correct 35\n",
      "size 37\n",
      "Percentage of majority class: 94.5945945945946 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "kmeans_train(k_mnist, it_mnist, x_mnist, y_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do this for 10, 100, 1000 iterations\n",
    "##### What is the majority class of each cluster?\n",
    "##### What is the percentage of the majority class in each cluster?\n",
    "##### Does each number have a cluster?\n",
    "##### If not, which hasn’t?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D MNIST understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some numbers are more difficult to predict with KNN and to cluster with K-means. Show why.  Start with examples, and get more general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E KNN and K-means CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "cifar = tf.keras.datasets.cifar10.load_data()[0]\n",
    "x_cifar, y_cifar = cifar[0], cifar[1]\n",
    "\n",
    "# Convert to grayscale\n",
    "x_cifar = np.dot(x_cifar[...,:3], [0.299, 0.587, 0.114])\n",
    "print(x_cifar.shape)\n",
    "\n",
    "n_x_cifar = x_cifar.shape[0]\n",
    "m_x_cifar = x_cifar.shape[1]**2\n",
    "\n",
    "x_cifar = x_cifar.reshape([n_x_cifar, m_x_cifar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset\n",
    "idx = np.random.randint(n_x_cifar, size=1030)\n",
    "x_cifar_sample = x_cifar[idx]\n",
    "y_cifar_sample = y_cifar[idx]\n",
    "\n",
    "# Create test & train set\n",
    "x_cifar_test = x_cifar_sample[:30]\n",
    "y_cifar_test = y_cifar_sample[:30]\n",
    "\n",
    "x_cifar_train = x_cifar_sample[30:]\n",
    "y_cifar_train = y_cifar_sample[30:]\n",
    "\n",
    "print(x_cifar_test.shape)\n",
    "print(y_cifar_test.shape)\n",
    "print(x_cifar_train.shape)\n",
    "print(y_cifar_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform cifar-10 to grayscale.\n",
    "* Does knn work similarly good?\n",
    "* Does k-means work similarly good?\n",
    "* Demonstrate this similar to B, C and D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F Linear regression BOSTON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a linear regression model on the BOSTON dataset to predict median\n",
    "values of houses. Test it on the test set. Show the code how you did it! What\n",
    "dependent variables (columns) have the biggest influence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G Logistik regression MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalute logistic regression as B and D on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much time did you work on this assignment and how often did you\n",
    "meet?\n",
    "* Did you use Google Colab or Anaconda?\n",
    "* What was your favourite excercise? Why?\n",
    "* What excercise did you like least? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
